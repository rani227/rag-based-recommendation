{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXA9JsSm/QSe7gK+4Uk+Mf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rani227/rag-based-recommendation/blob/main/shl_recommendation_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai pandas sentence-transformers scikit-learn faiss-cpu rapidfuzz python-dotenv beautifulsoup4 requests pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31hDcPR4wAoQ",
        "outputId": "1664cf53-cf76-4cf9-c08f-ca3d82684bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCd0KvNIurgO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import google.generativeai as genai\n",
        "from pydantic import BaseModel, Field, ValidationError, HttpUrl\n",
        "from typing import List, Optional, Dict, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DurationFilter(BaseModel):\n",
        "    min: Optional[int] = None\n",
        "    max: Optional[int] = None"
      ],
      "metadata": {
        "id": "Vch1z3tpv6vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParsedQuery(BaseModel):\n",
        "    skills: List[str] = Field(default_factory=list)\n",
        "    soft_skills: List[str] = Field(default_factory=list)\n",
        "    duration_minutes: Optional[DurationFilter] = None\n",
        "    adaptive_required: Optional[bool] = None\n",
        "    remote_required: Optional[bool] = None"
      ],
      "metadata": {
        "id": "fxsh0IsNx_Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AssessmentCatalogEntry(BaseModel):\n",
        "    name: str\n",
        "    url: HttpUrl\n",
        "    description: str\n",
        "    test_type: str\n",
        "    duration_minutes: int\n",
        "    remote_support: bool\n",
        "    adaptive_support: bool\n",
        "    embedding_input: str # This field is generated internally"
      ],
      "metadata": {
        "id": "sAOTTqMryJau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommendedAssessment(BaseModel):\n",
        "    assessment_name: str\n",
        "    assessment_url: HttpUrl\n",
        "    remote_testing_support: str # \"Yes\" or \"No\"\n",
        "    adaptive_irt_support: str # \"Yes\" or \"No\"\n",
        "    duration: str # e.g., \"30 minutes\"\n",
        "    test_type: str\n",
        "    relevance_reason: str # New field for traceability"
      ],
      "metadata": {
        "id": "ehF5oxhkyQfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommendationInput(BaseModel):\n",
        "    query: Optional[str] = None\n",
        "    job_description_url: Optional[HttpUrl] = None\n",
        "    num_recommendations: int = Field(default=10, ge=1, le=10)\n",
        "\n",
        "    # Custom validator to ensure at least one of query or job_description_url is provided\n",
        "    @classmethod\n",
        "    def __pydantic_validator__(cls, value):\n",
        "        if not isinstance(value, dict):\n",
        "            raise ValueError(\"Input must be a dictionary\")\n",
        "        if not value.get(\"query\") and not value.get(\"job_description_url\"):\n",
        "            raise ValueError(\"Either 'query' or 'job_description_url' must be provided.\")\n",
        "        return value"
      ],
      "metadata": {
        "id": "TOEO_a_ByUnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    data_df = pd.read_csv('shl_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'shl_dataset.csv' not found.\")"
      ],
      "metadata": {
        "id": "GUhTm8GVyc-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['remote_support'] = data_df['remote_support'].apply(lambda x: x.lower() == 'yes')\n",
        "data_df['adaptive_support'] = data_df['adaptive_support'].apply(lambda x: x.lower() == 'yes')"
      ],
      "metadata": {
        "id": "CVu2BLjPy3lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[\"embedding_input\"] = data_df.apply(\n",
        "    lambda row: f\"{row['name']}. {row['description']}. Type: {row['test_type']}. Duration: {row['duration_minutes']} mins. Remote Support: {row['remote_support']}. Adaptive Support: {row['adaptive_support']}.\", axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "T3xVZNEsy_d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame rows to Pydantic models for type safety\n",
        "catalog_entries = [AssessmentCatalogEntry(**row.to_dict()) for index, row in data_df.iterrows()]\n",
        "# Create a mapping from FAISS index to original data_df index/Pydantic object\n",
        "# This is crucial for retrieving the full data after FAISS search\n",
        "catalog_map = {i: entry for i, entry in enumerate(catalog_entries)}"
      ],
      "metadata": {
        "id": "ZRJQrz8WzHaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "46d887e2b46d47729070476ffdc90105",
            "8390aa09ae9a4a7d9a426780dd43452b",
            "694ffd965b2a4afbb4f6dc1c35a37bfb",
            "681d81523b524d13b2c1216fe45f7005",
            "af42bfe631dd4dd1b3e834e1ae2cff5a",
            "020ae39c67d24115a748908a93374873",
            "a7d54ef7a8a146c9a995e1be448fce7d",
            "10677b5a0f474b4cba87b43b142e9956",
            "feca3394424b402b832c69bdfba4a84e",
            "4c49375b07b84e72b42d903bc6728c16",
            "67f21a00cf184a4dac63a72db330bc41",
            "42bede7fafa742d4b851fb13f0074e22",
            "577bea50112e4071b4b09122e69560b7",
            "3eec0d2924474d42be84130a856737de",
            "78e2a9940d4646a085d86f970ff2e24f",
            "45299152db234ba2be5ac33750ba6acd",
            "bc6545ddfa4b4fab90087c835f054f8f",
            "7211dd4ff78c43b38d272f2e3063eada",
            "4bf7669b737242c1b59fb7b84f2a641d",
            "22dde1e7af264b92829a1cc7f58b4343",
            "318712ac275441cdb1e215986bde30dc",
            "957f2cc9873c4f0588569ac7a9870f53",
            "446b8d2748c148bb89c1f6d805e2c457",
            "782cf025aff941a9b7e35bbe007b9958",
            "b0ff2f99daa64721b5b07930c43b62ca",
            "c7962f4e3aad4a25b09bbdf5711879ed",
            "a797034cc9bc4db68fd68772272dbdee",
            "a3c594611b0540a2b47ec2b22c7f9f1e",
            "5ee89dbb8c154109a166d6dd70a12e11",
            "36c2c298c8d4428985320a1b98348c27",
            "f36feb55efe34932b05da94f2d33791b",
            "a2f569a6e78b4fd4a7ca1ad534841845",
            "5d54d0d7b9d84237a74e86298dcc96f4",
            "ad8829dae927475ea73a28715244c548",
            "72df38548a604432b60828ce6d702301",
            "bd086d58014e45dd98fbba8f98ea3c21",
            "822fc61c41314646adde5977a86f5f6c",
            "2dab82c50efc4e9297b9319b125bee6d",
            "cf0ff33565584d37891760ae5a6f211d",
            "1a55ffd02d6e4e3f98afd136bb06b7a8",
            "977bdb5d9f1548de90b4da9b8a113d47",
            "24a4318d6bcd4da48e241d31f0edccfb",
            "4eb59e44f73944459f441ac0a10dd7ce",
            "87d95b9d29c04044a0ca36f79dff9e25",
            "538c188eaff14838923bc0e719651e21",
            "7d97deb693f24397adb31bb57eadd39a",
            "014f66a9e8134be1b15a397a440eb09d",
            "19f693304d064b6989dbef690514ec1e",
            "f10939eb841844fc9f477a6f748a0e9f",
            "a334596a5ee74f979aa54accb8d5b0ee",
            "7045595c9de54ef9b2379bf43234030e",
            "8a72c71dbc724e9b9ba8c48706f12f18",
            "2691ab6fa9f249e8a98ae16da987c411",
            "e86e8f4e2e7d4286a93765105555885f",
            "6f5de4e105654baab6976266e9eb9126",
            "c50edf8454224a248a285b6ba4e95ed7",
            "1e20d230d9e14e1a9b71fc145eb4166d",
            "a4325e006a2443e89bbb62b3c91139c0",
            "94cdd660176f4fcf977912865edb11a9",
            "0aaba357eed945f282b518710665b2a8",
            "aa06781f0bf3454d8529040c57631905",
            "d8f72b9df6d749f588caf91403ca6f3d",
            "aa0b63c543994b5eb43e6624a0e13a8e",
            "f96a3248f9f04c18b5d50d3e36a43efb",
            "ae14b999d1e74e8d8fd142a0d0d17c7c",
            "fbc3e2fbe3774971a7c8cdd33b57e602",
            "372fe64dbc6b4517833446b2d267e103",
            "f17a4cb2a4414974aa94ff0ab6b04b38",
            "a3a51776316247b0b69897ca0932a938",
            "dda8bfd4d523464290a64b12459652df",
            "2444496d925941e3982b7e217ccf93b8",
            "c16a5efa6f1146a19b2d9298a44335c4",
            "304be0d0508a4b7ca9e93169410452c3",
            "3edb4c1175ad49ce8f3dc208f5f7e897",
            "b0ed9848e9bc4c86a25df7dc3c9b29a5",
            "3ff596d23d7c4031bf5259fad945461f",
            "28df2804ea074b1ea8c3e4037afb89b1",
            "6a9f5e29d96948d0822e019a54b6501c",
            "fe6369b1fa5342e6be17e119d729feeb",
            "0cea012ddaf74a5bb589c4965e1d13fd",
            "833abf79db3443a08da4d18058196316",
            "8dabc1c43cd744a5a9226a9c0044d82c",
            "bc5a79f25aba4416805769fda9cd9b14",
            "5504d9800aac49c39a9dbb39fb5dd389",
            "dff8268c6c144b85ba6ccc6881bd0cdd",
            "99f16214822a42d680dc75ec51c1af3b",
            "65e48921ec7449b392c091e3ff1ebef1",
            "757d6190536847698661f972e649c4c4",
            "d40464d96b7d47fcb3f2a31722d5b8f9",
            "8471e755dea24b15b342cefa979ccf1d",
            "1764bdf8611f469382e471913b381c8d",
            "86a96dbae69646069ad43040be2d103b",
            "990b4653e4aa454780d7b325b0849990",
            "3c85b79bda1e4e598ac81e93076d3be3",
            "43cea7c8bfba49ee9f1952521c2971d5",
            "b06948a9dcd643f283846aba5da4fce4",
            "90ec7717e14d4a6dac2f73974573bf15",
            "77a72454a7d546c8978edadf351bfceb",
            "04eab50582fb4f45825210bdbe3d2f9d",
            "85314d4604294aa7b60eca9b688eceee",
            "0ed7a3a55b1442e8bc4e01e3d732b602",
            "34258ca895a4495980c04c64cb801581",
            "25b8a8b1d49f4e58ac528dd4a68f9c33",
            "0defc9c2327e4f99be9d77d760c1063e",
            "2767767bb8714539936db3bec433007c",
            "95a3e4e335a64eba85fdd820582d3c6e",
            "bf0e0d229b104154b894a73e242cbb5e",
            "ffd6fb19621742b69ac536cb3beb51d6",
            "6108e90e44e44ada9fa605cd826fa59f",
            "1ad1558d602a4bc69cc2016db199f527",
            "f5f8fac1dd004bd3abd5013beee82672",
            "6f10161258694d48bfd8a85f91c936f7",
            "a662147ea0444410a0650b3c7e757592",
            "f9459c1900a34bf2ae09701222dcdfee",
            "e70fc99b08744618babaa5ed50b7b6c4",
            "c6340f69c0d14a5187c953ffd8801d9e",
            "50e52df72163420fa95e86641a5e41f8",
            "ac0396bd8b3e417a989115ef38e0bc3d",
            "22642baac96e41d88f4e68aacf49bdde",
            "9832c1d0934d44e9937eb73696adec32",
            "6fa4eadcb1bc419b95c87285e17111b7"
          ]
        },
        "id": "9GkvmmzRzYVG",
        "outputId": "20cf32a5-6e07-4db1-dc4b-faaec069f35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46d887e2b46d47729070476ffdc90105"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42bede7fafa742d4b851fb13f0074e22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "446b8d2748c148bb89c1f6d805e2c457"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad8829dae927475ea73a28715244c548"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "538c188eaff14838923bc0e719651e21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c50edf8454224a248a285b6ba4e95ed7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "372fe64dbc6b4517833446b2d267e103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a9f5e29d96948d0822e019a54b6501c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d40464d96b7d47fcb3f2a31722d5b8f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85314d4604294aa7b60eca9b688eceee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f8fac1dd004bd3abd5013beee82672"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embedding_model.encode([entry.embedding_input for entry in catalog_entries], show_progress_bar=True)\n",
        "normalized_embeddings = normalize(embeddings, axis=1, norm='l2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ce13bc445fea48fa9cff6d1db2839c9b",
            "d3a82224f39c4d3ba36a0efd75aa4c09",
            "ad87ead1b38b4535a3886b0699274455",
            "5ddde8fa19524833b229ee1386792c15",
            "d118fcfe4d8b4194a16db6e18c314392",
            "44755633d5fe4e69accc8f8deff3d848",
            "a6f7160327c84bdb92a972e5008c74d4",
            "6f7ab12570be4d5b9313337307f03158",
            "f356cffc4e6342b09f52606a740accf4",
            "c6c69a6664e24fec8140f9c02fec3cee",
            "25d276da0e2046ad9d078eead921fcfd"
          ]
        },
        "id": "Ncv5VmpDzgpv",
        "outputId": "238da2e6-8440-4748-b103-0711ef157e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce13bc445fea48fa9cff6d1db2839c9b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating FAISS index...\")\n",
        "embedding_dim = normalized_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(embedding_dim)\n",
        "faiss_index.add(normalized_embeddings)\n",
        "print(\"FAISS index created and populated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDIV6pWmzvx5",
        "outputId": "28a53863-5146-4261-bd55-e25f86a3bc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating FAISS index...\n",
            "FAISS index created and populated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not found. Please set it in your Colab secrets.\")\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "SPa27nRQz0Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "oVofeAo315zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_query_with_llm_gemini(query: str) -> ParsedQuery:\n",
        "    \"\"\"\n",
        "    Uses Gemini LLM to parse a natural language query into structured filters.\n",
        "    Returns a ParsedQuery Pydantic object.\n",
        "    \"\"\"\n",
        "    few_shot_examples = \"\"\"\n",
        "Q: I'm looking for a Java test that assesses both technical and soft skills under 40 minutes.\n",
        "A:\n",
        "```json\n",
        "{\n",
        "  \"skills\": [\"java\"],\n",
        "  \"soft_skills\": [\"communication\", \"teamwork\"],\n",
        "  \"duration_minutes\": { \"max\": 40 },\n",
        "  \"adaptive_required\": false,\n",
        "  \"remote_required\": false\n",
        "}\n",
        "```\n",
        "\n",
        "Q: Need an adaptive test for Python developers that works well for remote hiring.\n",
        "A:\n",
        "```json\n",
        "{\n",
        "  \"skills\": [\"python\"],\n",
        "  \"soft_skills\": [],\n",
        "  \"duration_minutes\": null,\n",
        "  \"adaptive_required\": true,\n",
        "  \"remote_required\": true\n",
        "}\n",
        "```\n",
        "\n",
        "Q: I need a test for entry-level sales roles, focusing on communication, and it must support remote testing. Duration should be around 25 minutes.\n",
        "A:\n",
        "```json\n",
        "{\n",
        "  \"skills\": [\"sales\"],\n",
        "  \"soft_skills\": [\"communication\"],\n",
        "  \"duration_minutes\": { \"max\": 30, \"min\": 20 },\n",
        "  \"adaptive_required\": false,\n",
        "  \"remote_required\": true\n",
        "}\n",
        "```\n",
        "\n",
        "Q: Find me a test for senior managers, adaptive support is a must.\n",
        "A:\n",
        "```json\n",
        "{\n",
        "  \"skills\": [\"management\", \"leadership\"],\n",
        "  \"soft_skills\": [],\n",
        "  \"duration_minutes\": null,\n",
        "  \"adaptive_required\": true,\n",
        "  \"remote_required\": null\n",
        "}\n",
        "```\n",
        "\n",
        "Q: \"\"\" + query + \"\"\"\n",
        "A:\n",
        "\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Using gemini-1.5-flash for speed and cost-effectiveness\n",
        "\n",
        "    prompt_content = f\"\"\"\n",
        "You are an expert at extracting structured filters from hiring assessment queries.\n",
        "Extract the relevant information as a JSON object.\n",
        "Ensure the JSON is always valid and complete, even if some fields are null or empty lists.\n",
        "For duration_minutes, if a specific duration is mentioned, try to infer a reasonable min/max range if not explicitly stated, otherwise use null.\n",
        "For 'remote_required' and 'adaptive_required', infer 'true' or 'false' if explicitly mentioned, otherwise use 'null'.\n",
        "\n",
        "{few_shot_examples}\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt_content,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0,  # Keep temperature low for structured output\n",
        "                response_mime_type='application/json' # Explicitly request JSON output\n",
        "            )\n",
        "        )\n",
        "        content = response.text.strip()\n",
        "        parsed_dict = json.loads(content)\n",
        "        return ParsedQuery(**parsed_dict) # Validate with Pydantic\n",
        "    except (json.JSONDecodeError, ValidationError) as e:\n",
        "        print(f\"LLM response could not be parsed or validated as JSON: {e}\")\n",
        "        print(f\"Raw LLM content: {content}\")\n",
        "        return ParsedQuery() # Return empty ParsedQuery on failure\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Gemini API call: {e}\")\n",
        "        return ParsedQuery() # Return empty ParsedQuery on failure"
      ],
      "metadata": {
        "id": "v6TQES-M2yIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_url(url: str) -> bool:\n",
        "    \"\"\"Checks if a string is a valid URL.\"\"\"\n",
        "    try:\n",
        "        result = urlparse(url)\n",
        "        return all([result.scheme in ['http', 'https'], result.netloc])\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "23jy4YTc3dbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_url(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches content from a URL and extracts readable text.\n",
        "    Includes basic error handling and text cleaning.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, timeout=15, headers=headers) # Added headers and increased timeout\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Attempt to find common elements that hold main content\n",
        "        main_content_tags = ['article', 'main', 'div', 'p', 'span']\n",
        "        text_parts = []\n",
        "        for tag_name in main_content_tags:\n",
        "            for tag in soup.find_all(tag_name):\n",
        "                # Heuristic: only consider tags with a reasonable amount of text\n",
        "                # and avoid script/style tags\n",
        "                if tag.name not in ['script', 'style'] and len(tag.get_text(strip=True)) > 50:\n",
        "                    text_parts.append(tag.get_text(separator=' ', strip=True))\n",
        "\n",
        "        if not text_parts: # Fallback if specific tags don't yield much\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "        else:\n",
        "            text = ' '.join(text_parts)\n",
        "\n",
        "        # Further clean the text: remove excessive whitespace, newlines\n",
        "        text = ' '.join(text.split()).strip()\n",
        "\n",
        "        # Simple truncation for extremely long pages to avoid overwhelming LLM\n",
        "        if len(text) > 4000: # Limit to first 4000 characters for LLM processing\n",
        "            text = text[:4000] + \"...\" # Indicate truncation\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML from {url}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "SZIHieog3j9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_candidates(query_embedding: np.ndarray, parsed_filters: ParsedQuery, top_k: int = 20) -> List[AssessmentCatalogEntry]:\n",
        "    \"\"\"\n",
        "    Retrieves candidates using FAISS and applies hard filters based on parsed query.\n",
        "    Returns a list of AssessmentCatalogEntry objects.\n",
        "    \"\"\"\n",
        "    # Normalize query embedding for dot product similarity (FAISS IndexFlatIP expects L2 normalized vectors)\n",
        "    query_embedding = normalize(query_embedding, axis=1, norm='l2')\n",
        "    scores, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    filtered_entries = []\n",
        "    for idx, score in zip(indices[0], scores[0]):\n",
        "        # Retrieve the original Pydantic object using the FAISS index\n",
        "        entry = catalog_map.get(idx)\n",
        "        if not entry:\n",
        "            continue # Should not happen if catalog_map is correctly built\n",
        "\n",
        "        # Apply hard filters dynamically using Pydantic model attributes\n",
        "        if parsed_filters.duration_minutes:\n",
        "            if parsed_filters.duration_minutes.max is not None and entry.duration_minutes > parsed_filters.duration_minutes.max:\n",
        "                continue\n",
        "            if parsed_filters.duration_minutes.min is not None and entry.duration_minutes < parsed_filters.duration_minutes.min:\n",
        "                continue\n",
        "\n",
        "        if parsed_filters.adaptive_required is not None:\n",
        "            if parsed_filters.adaptive_required and not entry.adaptive_support:\n",
        "                continue\n",
        "            if not parsed_filters.adaptive_required and entry.adaptive_support:\n",
        "                continue\n",
        "\n",
        "        if parsed_filters.remote_required is not None:\n",
        "            if parsed_filters.remote_required and not entry.remote_support:\n",
        "                continue\n",
        "            if not parsed_filters.remote_required and entry.remote_support:\n",
        "                continue\n",
        "\n",
        "        filtered_entries.append(entry) # Append the Pydantic object\n",
        "\n",
        "    return filtered_entries"
      ],
      "metadata": {
        "id": "GCmARo4w4Zoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reasons_with_llm(\n",
        "    query_text: str,\n",
        "    parsed_filters: ParsedQuery,\n",
        "    candidate_assessments: List[AssessmentCatalogEntry]\n",
        ") -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Uses Gemini LLM to generate relevance reasons for a list of candidate assessments.\n",
        "    \"\"\"\n",
        "    if not candidate_assessments:\n",
        "        return []\n",
        "\n",
        "    assessments_info = []\n",
        "    for i, ass in enumerate(candidate_assessments):\n",
        "        assessments_info.append(\n",
        "            f\"Assessment {i+1}:\\n\"\n",
        "            f\"  Name: {ass.name}\\n\"\n",
        "            f\"  Description: {ass.description}\\n\"\n",
        "            f\"  Type: {ass.test_type}\\n\"\n",
        "            f\"  Duration: {ass.duration_minutes} mins\\n\"\n",
        "            f\"  Remote Support: {'Yes' if ass.remote_support else 'No'}\\n\"\n",
        "            f\"  Adaptive Support: {'Yes' if ass.adaptive_support else 'No'}\\n\"\n",
        "        )\n",
        "    assessments_str = \"\\n\\n\".join(assessments_info)\n",
        "\n",
        "    # Include extracted skills in the prompt for better reasoning\n",
        "    skills_str = \"\"\n",
        "    if parsed_filters.skills:\n",
        "        skills_str += f\"Technical skills mentioned: {', '.join(parsed_filters.skills)}.\\n\"\n",
        "    if parsed_filters.soft_skills:\n",
        "        skills_str += f\"Soft skills mentioned: {', '.join(parsed_filters.soft_skills)}.\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Given the user's query/job description:\n",
        "\"{query_text}\"\n",
        "\n",
        "And the extracted key requirements:\n",
        "{json.dumps(parsed_filters.dict(), indent=2)}\n",
        "\n",
        "{skills_str}\n",
        "\n",
        "Here are some potentially relevant SHL assessments:\n",
        "{assessments_str}\n",
        "\n",
        "Your task is to review these assessments and for each one, provide a concise reason (1-2 sentences) explaining its relevance to the user's query, specifically highlighting how it addresses the mentioned skills or requirements.\n",
        "If an assessment is not relevant, do not include it in the output.\n",
        "Output a JSON array where each object has \"assessment_name\" and \"relevance_reason\".\n",
        "Ensure the JSON is always valid.\n",
        "\n",
        "Example Output Format:\n",
        "```json\n",
        "[\n",
        "  {{\n",
        "    \"assessment_name\": \"Assessment Name 1\",\n",
        "    \"relevance_reason\": \"Reason for relevance 1.\"\n",
        "  }},\n",
        "  {{\n",
        "    \"assessment_name\": \"Assessment Name 2\",\n",
        "    \"relevance_reason\": \"Reason for relevance 2.\"\n",
        "  }}\n",
        "]\n",
        "```\n",
        "\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash') # Or 'gemini-1.5-pro' for higher quality reasons\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                temperature=0.2, # A bit higher temperature for more natural language reasons\n",
        "                response_mime_type='application/json'\n",
        "            )\n",
        "        )\n",
        "        content = response.text.strip()\n",
        "        reasons_list = json.loads(content)\n",
        "        return reasons_list\n",
        "    except (json.JSONDecodeError, ValidationError) as e:\n",
        "        print(f\"LLM response for reasons could not be parsed or validated as JSON: {e}\")\n",
        "        print(f\"Raw LLM content: {content}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Gemini API call for reasons: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "4jjp2f4R4oey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(input_data: RecommendationInput) -> List[RecommendedAssessment]:\n",
        "    \"\"\"\n",
        "    Main function to get assessment recommendations.\n",
        "    Accepts a Pydantic RecommendationInput object.\n",
        "    Returns at most `num_recommendations` (min 1) as a list of RecommendedAssessment Pydantic objects.\n",
        "    \"\"\"\n",
        "    # Validate input using Pydantic model\n",
        "    try:\n",
        "        validated_input = RecommendationInput(**input_data.model_dump())\n",
        "    except ValidationError as e:\n",
        "        print(f\"Input validation error: {e.errors()}\")\n",
        "        return [] # Return empty list on validation failure\n",
        "\n",
        "    query_text = validated_input.query\n",
        "    if validated_input.job_description_url:\n",
        "        print(f\"Fetching text from URL: {validated_input.job_description_url}\")\n",
        "        extracted_text = get_text_from_url(str(validated_input.job_description_url)) # Convert HttpUrl to str\n",
        "        if not extracted_text:\n",
        "            print(\"Could not extract sufficient text from the provided URL.\")\n",
        "            return [] # Return empty list if URL text extraction fails\n",
        "        query_text = extracted_text\n",
        "\n",
        "    if not query_text:\n",
        "        print(\"No valid query text could be derived from the input.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nProcessing query: {query_text[:100]}...\") # Print first 100 chars for brevity\n",
        "\n",
        "    # Step 1: Parse query with LLM to extract structured filters\n",
        "    parsed_query = parse_query_with_llm_gemini(query_text)\n",
        "    print(f\"✅ Parsed Filters: {json.dumps(parsed_query.model_dump(), indent=2)}\")\n",
        "\n",
        "    # Step 2: Embed the query text\n",
        "    query_embedding = embedding_model.encode([query_text])\n",
        "\n",
        "    # Step 3: Retrieve candidates using FAISS and apply hard filters\n",
        "    # Retrieve more candidates than needed for better re-ranking potential\n",
        "    candidates = retrieve_candidates(query_embedding, parsed_query, top_k=validated_input.num_recommendations * 3)\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"⚠️ No matching assessments found after filtering. Attempting fallback to top semantic match.\\n\")\n",
        "        scores, indices = faiss_index.search(normalize(query_embedding, axis=1), 1)\n",
        "        if indices.size > 0:\n",
        "            fallback_entry = catalog_map.get(indices[0][0])\n",
        "            if fallback_entry:\n",
        "                # Generate a generic reason for the fallback\n",
        "                reasons_map = {fallback_entry.name: \"This is the closest semantic match found in the catalog based on your query, even if it didn't meet all specific filters.\"}\n",
        "                final_ranked_candidates = [fallback_entry]\n",
        "            else:\n",
        "                final_ranked_candidates = []\n",
        "                reasons_map = {}\n",
        "        else:\n",
        "            final_ranked_candidates = []\n",
        "            reasons_map = {}\n",
        "    else:\n",
        "        # Step 4: LLM-based Re-ranking and Reason Generation\n",
        "        print(f\"Generating reasons for {len(candidates)} candidates...\")\n",
        "        reasons_list = generate_reasons_with_llm(query_text, parsed_query, candidates)\n",
        "\n",
        "        reasons_map = {item['assessment_name']: item['relevance_reason'] for item in reasons_list}\n",
        "\n",
        "        # Filter candidates to only include those for which LLM provided a reason\n",
        "        # And sort them based on the order provided by the LLM in reasons_list (if applicable)\n",
        "        # For simplicity, we'll just take the top candidates that have a reason\n",
        "        # A more complex re-ranking would involve the LLM explicitly sorting.\n",
        "\n",
        "        # Create a list of (assessment, original_semantic_score) for sorting\n",
        "        temp_candidates_with_scores = []\n",
        "        for idx, score in zip(faiss_index.search(normalize(query_embedding, axis=1), validated_input.num_recommendations * 3)[1][0], faiss_index.search(normalize(query_embedding, axis=1), validated_input.num_recommendations * 3)[0][0]):\n",
        "            entry = catalog_map.get(idx)\n",
        "            if entry and entry in candidates and entry.name in reasons_map:\n",
        "                temp_candidates_with_scores.append((entry, score))\n",
        "\n",
        "        # Sort by original semantic score, then take top N\n",
        "        final_ranked_candidates = [item[0] for item in sorted(temp_candidates_with_scores, key=lambda x: x[1], reverse=True)[:validated_input.num_recommendations]]\n",
        "\n",
        "        # If LLM didn't return enough reasons, fill with top semantic matches that passed hard filters\n",
        "        if len(final_ranked_candidates) < validated_input.num_recommendations and len(candidates) > 0:\n",
        "            for cand in candidates:\n",
        "                if cand not in final_ranked_candidates and cand.name not in reasons_map:\n",
        "                    # Add a generic reason for these if LLM didn't process them\n",
        "                    reasons_map[cand.name] = \"This assessment is semantically relevant and meets your hard filters.\"\n",
        "                    final_ranked_candidates.append(cand)\n",
        "                if len(final_ranked_candidates) >= validated_input.num_recommendations:\n",
        "                    break\n",
        "\n",
        "        # Ensure minimum 1 recommendation, even if LLM gives no reasons\n",
        "        if not final_ranked_candidates and len(data_df) > 0:\n",
        "            print(\"Fallback: LLM provided no reasons. Returning top semantic match that passed hard filters, or general fallback.\")\n",
        "            if candidates: # If there were candidates that passed hard filters\n",
        "                final_ranked_candidates = [candidates[0]]\n",
        "                reasons_map[candidates[0].name] = \"This is the top semantically relevant assessment that met your specified filters.\"\n",
        "            else: # If no candidates passed hard filters, use the global semantic fallback\n",
        "                scores, indices = faiss_index.search(normalize(query_embedding, axis=1), 1)\n",
        "                if indices.size > 0:\n",
        "                    fallback_entry = catalog_map.get(indices[0][0])\n",
        "                    if fallback_entry:\n",
        "                        final_ranked_candidates = [fallback_entry]\n",
        "                        reasons_map[fallback_entry.name] = \"This is the closest semantic match found in the catalog based on your query, even if it didn't meet all specific filters.\"\n",
        "\n",
        "\n",
        "    output_data: List[RecommendedAssessment] = []\n",
        "    for ass_entry in final_ranked_candidates:\n",
        "        reason = reasons_map.get(ass_entry.name, \"No specific reason generated, but deemed relevant by the system.\")\n",
        "        try:\n",
        "            output_data.append(RecommendedAssessment(\n",
        "                assessment_name=ass_entry.name,\n",
        "                assessment_url=ass_entry.url,\n",
        "                remote_testing_support=\"Yes\" if ass_entry.remote_support else \"No\",\n",
        "                adaptive_irt_support=\"Yes\" if ass_entry.adaptive_support else \"No\",\n",
        "                duration=f\"{ass_entry.duration_minutes} minutes\",\n",
        "                test_type=ass_entry.test_type,\n",
        "                relevance_reason=reason\n",
        "            ))\n",
        "        except ValidationError as e:\n",
        "            print(f\"Validation error for RecommendedAssessment: {e.errors()} for entry: {ass_entry.name}\")\n",
        "            continue # Skip this entry if it fails validation\n",
        "\n",
        "    # Ensure min 1 recommendation if catalog is not empty\n",
        "    if not output_data and len(data_df) > 0:\n",
        "        print(\"Final fallback: No recommendations generated. Returning a general assessment.\")\n",
        "        fallback_entry = catalog_entries[0] # Pick the first entry as a last resort\n",
        "        output_data.append(RecommendedAssessment(\n",
        "            assessment_name=fallback_entry.name,\n",
        "            assessment_url=fallback_entry.url,\n",
        "            remote_testing_support=\"Yes\" if fallback_entry.remote_support else \"No\",\n",
        "            adaptive_irt_support=\"Yes\" if fallback_entry.adaptive_support else \"No\",\n",
        "            duration=f\"{fallback_entry.duration_minutes} minutes\",\n",
        "            test_type=fallback_entry.test_type,\n",
        "            relevance_reason=\"This is a general assessment provided as a fallback.\"\n",
        "        ))\n",
        "    elif not output_data and len(data_df) == 0:\n",
        "        print(\"Error: Product catalog is empty. Cannot provide recommendations.\")\n",
        "        return [] # Return empty if no data at all\n",
        "\n",
        "    return output_data"
      ],
      "metadata": {
        "id": "VeEIXCPA40Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Testing with Natural Language Query (Java, communication, adaptive, <30min) ---\")\n",
        "input1 = RecommendationInput(\n",
        "    query=\"I need an assessment for advanced Java developers that also evaluates their ability to communicate and work in teams. Preferably under 30 minutes and adaptive.\",\n",
        "    num_recommendations=5\n",
        ")\n",
        "recommendations1 = get_recommendations(input1)\n",
        "print(\"\\nRecommended Assessments (Query 1):\")\n",
        "print(json.dumps([r.model_dump() for r in recommendations1], indent=2, default=str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "qNxljLow499D",
        "outputId": "36417e91-8cfc-408b-b866-c4f5e1ac90d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing with Natural Language Query (Java, communication, adaptive, <30min) ---\n",
            "\n",
            "Processing query: I need an assessment for advanced Java developers that also evaluates their ability to communicate a...\n",
            "✅ Parsed Filters: {\n",
            "  \"skills\": [\n",
            "    \"java\"\n",
            "  ],\n",
            "  \"soft_skills\": [\n",
            "    \"communication\",\n",
            "    \"teamwork\"\n",
            "  ],\n",
            "  \"duration_minutes\": {\n",
            "    \"min\": null,\n",
            "    \"max\": 30\n",
            "  },\n",
            "  \"adaptive_required\": true,\n",
            "  \"remote_required\": null\n",
            "}\n",
            "⚠️ No matching assessments found after filtering. Attempting fallback to top semantic match.\n",
            "\n",
            "\n",
            "Recommended Assessments (Query 1):\n",
            "[\n",
            "  {\n",
            "    \"assessment_name\": \"Core Java (Entry Level) (New)\",\n",
            "    \"assessment_url\": \"https://www.shl.com/solutions/products/product-catalog/view/core-java-entry-level-new/\",\n",
            "    \"remote_testing_support\": \"Yes\",\n",
            "    \"adaptive_irt_support\": \"No\",\n",
            "    \"duration\": \"15 minutes\",\n",
            "    \"test_type\": \"Knowledge & Skills\",\n",
            "    \"relevance_reason\": \"This is the closest semantic match found in the catalog based on your query, even if it didn't meet all specific filters.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    }
  ]
}